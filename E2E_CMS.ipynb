{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOU2Ae8TIxLg6D9feTeQMLD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sidhu2690/E2E-CMS/blob/main/E2E_CMS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Importing all required libraries"
      ],
      "metadata": {
        "id": "1Yw42T7QkhFb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX3OUbrhkSj7"
      },
      "outputs": [],
      "source": [
        "pip install pyarrow\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Downloading the data"
      ],
      "metadata": {
        "id": "5QwY3Vsbko76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://cernbox.cern.ch/remote.php/dav/public-files/ZUHveJKajnZNwTA/QCDToGGQQ_IMGjet_RH1all_jet0_run0_n215556.train.snappy.parquet"
      ],
      "metadata": {
        "id": "vMXp4oT4kURY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Converting parquet to image files"
      ],
      "metadata": {
        "id": "qccAmQqhku0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pyarrow.parquet as pq\n",
        "import os\n",
        "\n",
        "start_time = time.time()\n",
        "def generate(pf, path, ab):\n",
        "    record_batch = pf.iter_batches(batch_size=4*1024)\n",
        "    while True:\n",
        "        try:\n",
        "            batch = next(record_batch)\n",
        "            ab = transform(batch, path, ab)\n",
        "\n",
        "        except StopIteration:\n",
        "            return ab\n",
        "\n",
        "def transform(batch, path, ab):\n",
        "    p = batch.to_pandas()\n",
        "    im = np.array(np.array(np.array(p.iloc[:, 0].tolist()).tolist()).tolist())\n",
        "    meta = np.array(p.iloc[:, 3])\n",
        "    return saver(im, meta, path, ab)\n",
        "\n",
        "def saver(im, meta, path, ab):\n",
        "    alpha, beta = ab\n",
        "\n",
        "    im[im < 1.e-3] = 0  # Zero_suppression\n",
        "    im[:, 0, :, :] = (im[:, 0, :, :] - im[:, 0, :, :].mean()) / (im[:, 0, :, :].std())\n",
        "    im[:, 1, :, :] = (im[:, 1, :, :] - im[:, 1, :, :].mean()) / (im[:, 1, :, :].std())\n",
        "    im[:, 2, :, :] = (im[:, 2, :, :] - im[:, 2, :, :].mean()) / (im[:, 2, :, :].std())\n",
        "\n",
        "    for i in range(meta.shape[0]):\n",
        "        img = im[i, :, :, :]\n",
        "\n",
        "        channel1 = img[0, :, :]\n",
        "        channel2 = img[1, :, :]\n",
        "        channel3 = img[2, :, :]\n",
        "\n",
        "        channel1 = np.clip(channel1, 0, 500 * channel1.std())\n",
        "        channel2 = np.clip(channel2, 0, 500 * channel2.std())\n",
        "        channel3 = np.clip(channel3, 0, 500 * channel3.std())\n",
        "\n",
        "        channel1 = 255 * (channel1) / (channel1.max())\n",
        "        channel2 = 255 * (channel2) / (channel2.max())\n",
        "        channel3 = 255 * (channel3) / (channel3.max())\n",
        "\n",
        "        img[0, :, :] = channel1\n",
        "        img[1, :, :] = channel2\n",
        "        img[2, :, :] = channel3\n",
        "\n",
        "        img = img.astype(np.uint8)\n",
        "        img = img.T\n",
        "\n",
        "        if meta[i] == 0:\n",
        "            impath = os.path.join(path, \"0\", str(str(alpha) + \".png\"))\n",
        "            alpha = alpha + 1\n",
        "        if meta[i] == 1:\n",
        "            impath = os.path.join(path, \"1\", str(str(beta) + \".png\"))\n",
        "            beta = beta + 1\n",
        "\n",
        "        cv2.imwrite(impath, img)\n",
        "\n",
        "    return [alpha, beta]\n",
        "\n",
        "def runner(source, target):\n",
        "    ab = [0, 0]\n",
        "\n",
        "    os.makedirs(os.path.join(target, \"1\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(target, \"0\"), exist_ok=True)\n",
        "\n",
        "    ab = generate(pq.ParquetFile(source), target, ab)\n",
        "\n",
        "    print(\"The files were successfully generated\")\n",
        "\n",
        "parquet_file_name = 'QCDToGGQQ_IMGjet_RH1all_jet0_run0_n215556.train.snappy.parquet'\n",
        "parquet_file_path = f'/kaggle/working/{parquet_file_name}'\n",
        "output_directory = '/kaggle/working/output/'\n",
        "\n",
        "runner(source=parquet_file_path, target=output_directory)\n",
        "end_time = time.time()\n",
        "Running_time = end_time - start_time\n",
        "print(f\"Time: {Running_time} seconds\")"
      ],
      "metadata": {
        "id": "gmmjFsUJkWhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Simple CNN model"
      ],
      "metadata": {
        "id": "LDYim0O1k6BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n",
        "\n",
        "image_size = (128, 128)\n",
        "num_classes = 1  # Binary classification\n",
        "\n",
        "cnn_model_bn_with_dropout = Sequential()\n",
        "\n",
        "cnn_model_bn_with_dropout.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_size[0], image_size[1], 3)))\n",
        "cnn_model_bn_with_dropout.add(BatchNormalization())\n",
        "cnn_model_bn_with_dropout.add(MaxPooling2D(2, 2))\n",
        "cnn_model_bn_with_dropout.add(Dropout(0.25))  # Adding dropout with a dropout rate of 0.25\n",
        "\n",
        "cnn_model_bn_with_dropout.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "cnn_model_bn_with_dropout.add(BatchNormalization())\n",
        "cnn_model_bn_with_dropout.add(MaxPooling2D(2, 2))\n",
        "cnn_model_bn_with_dropout.add(Dropout(0.25))  # Adding dropout with a dropout rate of 0.25\n",
        "\n",
        "cnn_model_bn_with_dropout.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "cnn_model_bn_with_dropout.add(BatchNormalization())\n",
        "cnn_model_bn_with_dropout.add(MaxPooling2D(2, 2))\n",
        "cnn_model_bn_with_dropout.add(Dropout(0.25))  # Adding dropout with a dropout rate of 0.25\n",
        "\n",
        "cnn_model_bn_with_dropout.add(Conv2D(256, (3, 3), activation='relu'))\n",
        "cnn_model_bn_with_dropout.add(BatchNormalization())\n",
        "cnn_model_bn_with_dropout.add(MaxPooling2D(2, 2))\n",
        "cnn_model_bn_with_dropout.add(Dropout(0.25))  # Adding dropout with a dropout rate of 0.25\n",
        "\n",
        "cnn_model_bn_with_dropout.add(Flatten())\n",
        "\n",
        "cnn_model_bn_with_dropout.add(Dense(512, activation='relu'))\n",
        "cnn_model_bn_with_dropout.add(BatchNormalization())\n",
        "cnn_model_bn_with_dropout.add(Dropout(0.5))  # Adding dropout with a dropout rate of 0.5\n",
        "\n",
        "cnn_model_bn_with_dropout.add(Dense(256, activation='relu'))\n",
        "cnn_model_bn_with_dropout.add(BatchNormalization())\n",
        "cnn_model_bn_with_dropout.add(Dropout(0.5))  # Adding dropout with a dropout rate of 0.5\n",
        "\n",
        "cnn_model_bn_with_dropout.add(Dense(num_classes, activation='sigmoid'))\n",
        "\n",
        "cnn_model_bn_with_dropout.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "BymgHgrwk0Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Callbacks\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "class LossCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        train_loss.append(logs['loss'])\n",
        "        val_loss.append(logs['val_loss'])\n",
        "loss_callback = LossCallback()"
      ],
      "metadata": {
        "id": "X3OPtqhmk__A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Train the model for 60 epochs"
      ],
      "metadata": {
        "id": "4FepufedlE3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = cnn_model_bn.fit(\n",
        "    x=tf.keras.utils.image_dataset_from_directory(\n",
        "        '/kaggle/working/output/',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        image_size=image_size,\n",
        "        batch_size=32,\n",
        "        seed=42,\n",
        "    ),\n",
        "    epochs=60,\n",
        "    validation_data=tf.keras.utils.image_dataset_from_directory(\n",
        "        '/kaggle/working/output/',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        image_size=image_size,\n",
        "        batch_size=32,\n",
        "        seed=42,\n",
        "    ),\n",
        "    callbacks=[loss_callback]\n",
        ")"
      ],
      "metadata": {
        "id": "PMkjd8qilDoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Training curves"
      ],
      "metadata": {
        "id": "U-LUOFgElQ6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_loss, label='Training Loss', marker='o')\n",
        "plt.plot(val_loss, label='Validation Loss', marker='o', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qPBfwT-klOwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model-2"
      ],
      "metadata": {
        "id": "ay56yY5slVxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n",
        "\n",
        "image_size = (128, 128)\n",
        "num_classes = 1  # Binary classification\n",
        "\n",
        "cnn_model_2 = Sequential()\n",
        "\n",
        "cnn_model_2.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_size[0], image_size[1], 3)))\n",
        "cnn_model_2.add(BatchNormalization())\n",
        "cnn_model_2.add(MaxPooling2D(2, 2))\n",
        "cnn_model_2.add(Dropout(0.25))\n",
        "cnn_model_2.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "cnn_model_2.add(BatchNormalization())\n",
        "cnn_model_2.add(MaxPooling2D(2, 2))\n",
        "cnn_model_2.add(Dropout(0.25))\n",
        "cnn_model_2.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "cnn_model_2.add(BatchNormalization())\n",
        "cnn_model_2.add(MaxPooling2D(2, 2))\n",
        "cnn_model_2.add(Dropout(0.25))\n",
        "cnn_model_2.add(Conv2D(256, (3, 3), activation='relu'))\n",
        "cnn_model_2.add(BatchNormalization())\n",
        "cnn_model_2.add(MaxPooling2D(2, 2))\n",
        "cnn_model_2.add(Dropout(0.25))\n",
        "cnn_model_2.add(Flatten())\n",
        "cnn_model_2.add(Dense(512, activation='relu'))\n",
        "cnn_model_2.add(BatchNormalization())\n",
        "cnn_model_2.add(Dropout(0.5))\n",
        "cnn_model_2.add(Dense(256, activation='relu'))\n",
        "cnn_model_2.add(BatchNormalization())\n",
        "cnn_model_2.add(Dropout(0.5))\n",
        "cnn_model_2.add(Dense(num_classes, activation='sigmoid'))\n",
        "\n",
        "cnn_model_2.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "FzHlO9bplZ3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = cnn_model_bn.fit(\n",
        "    x=tf.keras.utils.image_dataset_from_directory(\n",
        "        '/kaggle/working/output/',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        image_size=image_size,\n",
        "        batch_size=32,\n",
        "        seed=42,\n",
        "    ),\n",
        "    epochs=60,\n",
        "    validation_data=tf.keras.utils.image_dataset_from_directory(\n",
        "        '/kaggle/working/output/',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        image_size=image_size,\n",
        "        batch_size=32,\n",
        "        seed=42,\n",
        "    ),\n",
        "    callbacks=[loss_callback]\n",
        ")"
      ],
      "metadata": {
        "id": "x7U550CGmCio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_loss, label='Training Loss', marker='o')\n",
        "plt.plot(val_loss, label='Validation Loss', marker='o', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FWIEP817mEuZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}